{"/pages/design/draft/": {
    "title": "Design Draft",
    "keywords": "Jekyll",
    "url": "/pages/design/draft/",
    "body": "This is an draft page."
  },"/pages/about/": {
    "title": "About",
    "keywords": "Jekyll",
    "url": "/pages/about/",
    "body": "About-me"
  },"/pages/contact/": {
    "title": "Contact",
    "keywords": "Jekyll",
    "url": "/pages/contact/",
    "body": "Email : icetime963@gmail.com Github : https://github.com/bingsu-kun"
  },"/back-end-study/2021-06-28-Elastic-Search.html": {
    "title": "검색 속도 향상을 위한 Elastic Search 도입",
    "keywords": "Back-End-Study",
    "url": "/back-end-study/2021-06-28-Elastic-Search.html",
    "body": "MQ까지 도입했는데… 특정 상황에서는 아직 서버가 죽네 ㅠㅠ 지금까지 포스팅을 해가며 대용량 트래픽 프로젝트에 MQ까지 도입했다. “이제 요청을 큐에 저장하며 순차적으로 처리하기 때문에 쓰레드가 꽉 차지만 않으면 문제없겠다!” 라고 생각하였다. 하지만 생각이 끝나고 몇 초 뒤… 쓰레드 자원이 모두 소진되어 팡팡 터지고 있는 서버가 상상되어 다시 키보드를 잡는다. 우선 문제의 저 쓰레드가 꽉 차는 상황에는 무엇이 있을지 위험분석을 해보도록 하자. 많은 양의 데이터를 응답으로 보내는 경우 지금까지 따로 명시하지는 않았지만 데이터로 불특정다수의 애니메이션 리뷰 댓글들을 크롤링하여 사용하고 있고 크기는 대략 50000개 정도이다. ‘애니’라는 단어로 게시글을 찾을 경우 많은 양의 데이터를 응답으로 보내게 되면서 latency가 길어지는 결과를 보였다. 실제로 쓰레드 자원이 모두 소진되어 요청이 거절되고 있는 것은 아니지만 더 많은 데이터를 가지고 있을 경우 그렇게 될 가능성은 짙다. 즉, 여기서 알 수 있는 것은 응답으로 보내는 데이터 양이 많아지면 많아질수록 쓰레드가 요청을 처리하는 시간이 길어지고 (응답까지 완료되어야 가용 가능한 쓰레드가 되기 때문에) 요청이 밀리면서 거절된다는 것을 예상할 수 있겠다. 그럼 데이터 양이 많아질수록 왜 시간이 길어지는 걸까? 내가 생각한 큰 이유는 두 가지이다. 네트워크가 느려서 데이터의 전송이 느리다. 요청에 맞는 데이터 색인에 시간이 오래걸린다. 전자의 경우는 네트워크 환경과 더 밀접하기 때문에 무료 평가판을 사용하고있는 현재 상황에서는 최선이라 생각한다. 클라우드 쪽에서 네트워크와 관련된 설정으로는 MQ를 도입하면서 리전의 거리에 따른 latency 증가를 확인하고 해결해놓은 상태이다. 이것 이외의 문제점이 발생한다면 그것은 구글 클라우드 내부에서의 문제이거나 클라이언트의 네트워크 환경일 것이라 생각한다. 그래서 더욱 중심을 두고 봐야할 부분은 후자이다. 데이터가 많을수록 색인에 시간이 오래걸릴것은 당연하다. 전자보다는 후자를 개선하는 방향으로 진행하기로 결정하며 이를 조금 더 완화하기위해 Elastic Search라는 툴을 도입하기로 했다. Elastic Search는 어떤 도구일까? Elastic Search란? Elastic Search(이하 ES)가 제공하는 기능 중 특징적으로 3가지 기능이 있다. Inverted Index (역색인) Shard (샤드) Replica (레플리카) 역 색인을 설명하기위해 먼저 보통의 RDBS 테이블과 비교할 필요가 있다. 보통 RDBS의 테이블은 키와 값으로 이루어져있다. 하지만 ES의 역 색인은 RDBS 테이블안의 값 내용 중 단어를 뽑아내어 키로 두고, 값에는 RDBS 테이블에서 값이 위치하는 키의 값을 넣는다. EX) RDBS Table key value 0 스타벅스 아메리카노 1 할리스 아메리카노 2 스타벅스 라떼 Inverted Index Table key value \"스타벅스\" 0,2 \"할리스\" 1 \"아메리카노\" 0,1 \"라떼\" 2 이렇게 정리된 역 색인 테이블에서 만약 “스타벅스”를 검색한다면 0번, 2번 데이터만 가져오면 되므로 따로 검색 알고리즘을 타고 데이터를 찾으러다닐 필요가 없어진다. 한글 형태소는 정확하게 구분이 안되므로 ES에 nori(노리)또는 arirang(아리랑)같은 분석기를 적용해야한다! 샤드는 간단히말해 저장된 데이터를 원하는만큼 쪼개어 여러 곳에 저장해놓는 것이다. ES 클러스터를 여러개 구동하게하여 이 쪼개진 문서들을 분산해서 검색하게 한다면 훨씬 빠르게 데이터를 검색할 수 있을 것이다. 하지만 무조건 샤드를 많이 만든다고 성능이 좋아지는 것은 아니니 최소한으로 쪼개고 최대한으로 효율을 낼 수 있는 선을 찾아서 샤드를 나누어놓자. 레플리카는 쪼개진 샤드를 몇 개 복제할지 정하는 것이다. 레플리카를 사용할 때의 이점은 예상치못하게 클러스터가 다운된 경우에도 서비스가 원활이 이루어질 수 있다. 레플리카를 만들면 키워드당 매칭되는 문서에 따라 검색 효율이 낮아지기도하지만 안정성이라는 큰 장점을 위해서는 조금 희생할 가치가 있다고 생각한다. Elastic Search의 단점 ES의 단점으로는 크게 세가지가 있다. 실시간 처리 불가 트랜잭션과 롤백을 제공하지 않음 데이터를 실제로 업데이트하지 않음 이 중 세번째 항목에 관해서 부가 설명을 하자면, 업데이트 기능은 있지만 기존의 데이터를 지우고 새로만드는 방식으로 업데이트한다. 따라서 실시간으로 입력되는 데이터들을 빠르게 색인해야하는 경우에는 ES가 크게 적합하다고 말할 수는 없다. Elastic Search를 도입하고… 클러스터는 4개, 샤드는 8개로 구성하여 진행한 결과, ‘애니’라는 키워드를 검색했을 때 기존 DB만을 이용했을 때보다 약 2초 정도 감소하었다. 뿐만 아니라 ‘애니’보다 latency가 짧았던 ‘명작’, ‘진짜’ 키워드를 검색했을때도 약 1초정도 감소한 결과를 보여주었다. 여기서 추가로 레플리카 수를 1로 해서 진행한 결과 ‘애니’ 키워드는 기존 DB만을 이용했을 때보다 조금 더 늘었지만 나머지는 비슷하게 유지되었다. 결론을 내리자면… 키워드와 매칭되는 문서가 많을수록 DB가 더 효율적일 수 있다. 클러스터와 샤드의 개수를 늘리면 늘릴수록 더욱 색인이 빨라진다. 레플리카는 안정성을 늘려주지만 매칭이 많은 키워드의 검색 효율은 낮춘다. 이 결론은 내가 테스트해본 환경에서의 결론이다. 데이터의 갯수와 내용에 따라 언제든지 결과가 달라질 수 있으므로 각자의 환경에서 테스트를 거쳐 도입을 결정해야한다!! 도입을 하고 느낀점은 “확실히 검색 속도를 낮출 수 있는 좋은 방법이다! 하지만 클러스터가 꽤 많아야되겠는데…?” 이다. ES의 겉만 핥아서 이보다 더 큰 효율을 낼 수 있는 ES의 기능에는 무엇이 있을지 아직 모르지만, 지금 상태로 ES와 클라우드를 합쳐서 사용하려면 비용쪽으로 많이 고려해봐야 할 것 같다는 소감이다. 또한 클라우드에서 클러스터로 사용하기위해 인스턴스를 띄워서 도커를 설치해 사용했는데, 단일 스택 클러스터만을 위한 제품을 사용한다면 비용이 어떻게 변할지 탐구해 봐야한다. 하지만 이번 프로젝트에서 비용적인 면은 무료 크레딧으로 해결하고 있기 때문에 당장 고려할 점이 아니므로 더 다루지 않기로 했다. 이것으로 ES 도입 포스팅을 마친다."
  },"/back-end-study/2021-06-13-Message-Queue.html": {
    "title": "더욱 원활한 DB 접근을 위해 MQ와 캐싱을 도입하자",
    "keywords": "Back-End-Study",
    "url": "/back-end-study/2021-06-13-Message-Queue.html",
    "body": "Message Queue를 사용하자! Message Queue (이하 MQ)는 웹 서버와 DB사이에 위치하여 서버에서 DB에 접근하는 요청을 받아 Queue에 임시로 저장해주고 차례로 처리할 수 있도록 도와주는 툴이다. 앞서 nginx를 활용한 Load Balancing을 통해 클라이언트가 서버에 접근하는 것 까지는 개선할 수 있었다. 하지만 웹 서버를 여러개 구동한다고 해도 결국에 데이터를 구하기위해 DB에 접근하게 되는데 이때 DB의 성능은 한계가 있고, DB개수를 늘린다고 해도 너무 많아지면 동기화때문에 데이터 무결성 유지가 힘들어진다. 그래서 DB에 향하는 요청을 중계해주기위해 등장한 것이 MQ이다. 좀 더 자세히 들어가보자. 만약 운영 중인 웹 서버가 3대이고, 요청 타임 아웃시간은 3초, DB가 한 개의 요청을 처리하는데 3초가 걸린다고 가정한다면, 이 DB는 갖다 버려야한다 세 서버에서 동시에 요청이 들어왔을 때 한 가지 요청밖에 응답하지 못한다. 그래서 MQ는 이렇게 요청이 몰릴 때 요청들을 Queue로 받아들인 후 클라이언트에게는 Queue 대기 순번을 응답해주고, DB에게는 순차적으로 몰렸던 요청들을 처리한다. 따라서 불필요하게 많은 클라이언트의 요청을 받을 필요도 없어지고 네트워크 불량으로인한 타임 아웃 처리와 요청의 램프업으로 인해 응답이 늦어지는 상황의 처리를 따로 할 수 있게되어서 효율과 편리 두 가지를 모두 잡을 수 있다고 할 수 있다. RabbitMQ 사용하기 Message Queue 중에서도 다른 개발자사이에 이름이 많이 오가고 많이 사용되는 RabbitMQ를 사용해보기로 했다. MQ를 처음 배워보기 때문에 아티클이 많은 툴이 공부하기 편할 것 같아서 선택하게 되었다. 우선 MQ를 사용할 땐 웹 서버에 Producer와 Consumer 라는 클래스를 별도로 만들어서 사용하게 된다. 웹 서버 소스코드에서 findById 같은 API 메서드를 제작하여 사용할 때 jpa나 jdbctemplate 인스턴스로 바로 접근하지않고 Producer 인스턴스를 만들어서 MQ로 요청이 가게끔 해준다. 다음 Consumer에 DB접근 인스턴스 메서드를 담아 리스너를 사용해서 Queue의 요청을 처리하게 하는 식이다. 추가적으로 DB는 PostgreSQL을 사용하였다. 아래는 Producer와 Consumer의 예시 코드이다. public class Producer { @Autowired private RabbitTemplate rabbitTemplate public void sendTo(String message) { this.rabbitTemplate.convertAndSend(\"큐 이름을 넣어주세요\",message); } } public class Consumer { @Autowired ObjectMapper objectMapper; @Autowired PostRepository postRepository; @RabbitListener(Queues = \"큐 이름을 넣어주세요\") // 큐에 작업이 있을 경우 메세지로 받아옴 public void handler(String message) throws JsonProcessingException { Post post = objectMapper.readValue(message, Post.class); //리스너를 통해 받아온 메세지를 post 클래스에 매핑. postRepository.save(post) } } 이렇게 Queue에 들어있는 요청을 처리하는 코드를 작성하였는데, 때에 따라서는 Queue가 있는 장비나 인스턴스가 문제를 일으키는 경우도 있어, 여러 Queue를 운영하기도 한다. 이 때 Queue는 동기화해서 사용한다. 추가적으로 주의해야할 점은 RabbitMQ를 운영하는 인스턴스에서 외부 포트를 열어줄 때, RabbitMQ 모니터링 포트와 Queue로 요청하는 포트 이렇게 두 포트를 열어주어야 한다. 그렇게해야 모니터링 포트로 접속하여 새로운 큐를 만들어 줄 수 있을 뿐더러 큐 관리까지 가능하다. 모니터링 페이지에 접속한 후에는 유저 로그인 창이 뜨는데 첫 접속은 아이디 guest, 비밀번호 guest로 해서 접속가능하고 들어가서 아이디를 새로 만들어주는 것을 추천한다. 캐싱을 해보자 ! 우리가 흔히 접속하는 사이트의 첫 페이지에는 꽤나 많은 양의 정보를 담고있는 경우가 많다. 예를들어 네이버의 경우 첫 페이지에 검색 창과 더불어 실시간으로 뉴스나 기사들을 볼 수 있는 링크가 즐비하다. 이와 관련된 내용들은 전부 DB에서 가져오는 것일테고, 모든 사용자가 이 페이지를 자주 요청하게 되면서 DB에 요청하는 데이터의 양과 횟수가 상당할 것이다. 당연히 DB입장에서는 부담이 될텐데 이를 웹 서버에서 미리가져와서 준비해놓고 있는다면? DB에 접근하지 않고도 페이지의 정보를 응답할 수 있기 때문에 DB도 훨씬 부담이 적어질테고 사용자 입장에서도 페이지를 띄우는 시간이 짧아져 편리할 것이다. 우리는 이를 캐싱이라 부르고 있는데 DB와 MQ에 가해지는 부하를 줄이기 위해 간단하게 만들어보았다. Scheduler를 통해 주기적으로 DB에서 첫 페이지와 관련된 데이터를 받아오고 유저가 그 데이터를 요청할 경우 캐싱한 데이터를 바로 보낸다. 이는 꼭 첫 페이지가 아니라도 자주 접근되어지는 데이터가 있다면 캐싱 가능하다. 간단하게 구현한 것이기 때문에 웹 서버 소스코드에서 구현하였지만, 일반적으로는 웹 서버 소스코드에 직접 구현하지 않고 Redis같은 캐시 서버 툴을 이용해서 캐싱한 데이터를 저장해 두었다가 필요할 때 접근해서 보낸다. 아래는 간단하게 구현한 코드이다. public class PostCacheService { @Autowired PostRepository postRepository; private Page&lt;Post&gt; firstPage; @Scheduled(cron = [크론식]) //크론식에 따라 작성해주면 주기적으로 데이터를 캐싱해옴. public void updateFirstPage() { firstPage = postRepository.findAll( PageRequest.of(0, 20, Sort.by(\"id\").aescending()) // 첫페이지에 필요한 데이터를 미리 캐싱. 예시에서는 첫페이지에서 모든 데이터를 20개로 쪼갠 page 중 0번 page를 오름차순으로 불러온다. ); } public Page&lt;Post&gt; getFirstPage() { retrun this.firstPage; } } 이 때 주의해야할 점이 반드시 main함수가 있는 클래스에 @EnableScheduling 어노테이션을 적어줘야 정상 작동한다. 여기까지 캐싱을 준비하고 스트레스 테스트를 해보면 확실히 응답률이 좋아지긴 했으나 한가지 더 해결해야할 숙제가 남아있었다. 클라우드를 이용한다면 Region에 주목하자 나는 GCP를 이용해서 이 프로젝트를 진행 중이다. 인스턴스가 적을 때에는 인스턴스간 거리와 그에따른 지연률을 크게 고려하지 않았으나, 점점 인스턴스가 많아지고 하나의 요청을 처리할 때 인스턴스 사이를 오가는 횟수가 많아지면서 인스턴스간 거리를 주목하게 되었다. 다음은 수정 전 인스턴스들의 리전이다. 대한민국 서울 - 웹 서버 워커노드 3대 대만 - nginx 일본 도쿄 - rabbitMQ, PostgreSQL DB 이 경우 요청의 플로우는 서울(클라이언트) -&gt; 대만(nginx) -&gt; 서울(worker node) -&gt; 도쿄(rabbitMQ) -&gt; 도쿄(PostgreSQL) -&gt; 도쿄(rabbitMQ) -&gt; 서울(worker node) -&gt; 대만(nginx) -&gt; 서울(클라이언트) 이다. 아시아 권역안에서만 움직이긴해도 시간으로 따지면 지연시간이 꽤 나온다. 이는 Postman에서 요청을 넣으면 확인해볼 수 있다. 나는 0.3s 정도의 지연시간이 나왔다. 이 시간을 줄이면 줄일수록 요청을 처리할 수 있는 빈 쓰레드가 빨리 생겨 많은 요청을 처리할 수 있게된다. 그러므로 nginx와 worker node들, rabbitMQ는 같은 리전에 두는것이 좋다. 따라서 이를 변경한 결과, 대한민국 서울 - (nginx + 웹 서버 워커노드) 1대, 웹 서버 워커노드 1대, rabbitMQ 1대 대만 - nginx 일본 도쿄 - PostgreSQL DB 이렇게 구성이 변경되었다. 이렇게 다시 지연시간을 테스트한 결과 0.12s 정도로 줄어들었다. 클라우드를 사용하고 있는 경우 가장 빠르고 확실하게 성능을 올릴 수 있는 좋은 방법일 것 같다. 주의사항 linux 서버로 nginx 구동 시 nofile limit 때문에 요청이 막히는 경우가 있다. (Too many open files 에러) 이 경우 prlimit 커맨드를 사용해서 한계를 늘려주어야 한다. 기본적으로 1000으로 되어 있는데 /proc/sys/fs/file-max를 cat 명령어로 확인해보면 대략 36만 까지 키울 수 있었다.(GCP medium 기준)"
  },"/back-end-study/2021-05-25-github-webhokk-jenkins.html": {
    "title": "github webhook과 jenkins로 배포 자동화하기",
    "keywords": "Back-End-Study",
    "url": "/back-end-study/2021-05-25-github-webhokk-jenkins.html",
    "body": "깃허브에 코드만 올렸을 뿐인데 자동으로 배포까지? 오늘은 배포 자동화에 대해 공부하면서 배운 내용을 정리해보고자 한다. 우리는 흔히 프로젝트의 소스코드들을 깃허브에 레포지토리를 만들어서 관리한다. 열심히 코딩해서 릴리즈를 배포해야 될 시기가 왔다고 할 때, 어떤 과정을 거쳐서 배포가 이루어지는지 먼저 살펴보자. Spring 프로젝트를 배포한다고 가정하면, 우선 수정된 사항을 브랜치에 push한다. 다음 로컬 환경에서 프로젝트를 빌드하고 빌드 후 생긴 jar나 war파일을 서버로 전송한다. 그 후 배포 스크립트를 통해서 배포한다. 하지만 이것은 개인 프로젝트일 때의 이야기이고, 실무에서는 중간에 테스트 서버에서 테스트를 거치고 QA까지 진행된 후 배포가 된다. 위 과정 중 push - build - 전송 - 배포 과정이 반복되는 경우가 많다. 이러한 과정을 github webhook과 jenkins를 이용하여 단축할 수 있다. 소스코드를 push만하면 github webhook을 통해 push된 사실을 감지하여 jenkins가 자동으로 코드를 build하고 배포까지 진행한다. Jenkins 빌드 구성 빌드 구성은 여느 일반 jenkins 사용과 다를 것이 없다. 우선 아이템을 만들고, 아이템 구성에서 소스코드 관리를 git으로 변경해준다. 다음 URL에 git 프로젝트 레포지토리 주소를 적어주고, 배포를 위한 브랜치가 따로 있다면 아래 branch를 적는 곳에 추가로 적어준다. 그리고 빌드유발에 github hook trigger를 체크해준다. 이를 체크해주어야 github webhook에 의한 빌드 시작이 가능해진다. 다음 빌드 항목에서 빌드 스텝을 생성해주고 Excute shell에 ./mvnw clean package를 입력한다. 이는 mvnw를 이용해서 target 폴더 clean을 진행 후 package 명령을 실행한다는 의미이다. 여기서 주의할 점이 있는데, 개발환경은 윈도우인데 배포환경이 Linux인 경우 애플리케이션 권한 체계가 다르므로 위 명령어 전에 chmod 544 ./mvnw를 통해 권한을 주어야 정상 작동한다. 여기까지 진행하면 코드를 push하면 jenkins에서 자동 build한다. 이제 배포를 자동화 해보자. Jenkins 배포 구성 빌드 된 후 실행되어야하는 파일을 지정해줘야 하므로 “빌드 후 조치” 항목에서 Transfer Set을 수정해주자. Source files를 target/~~~.jar 로 수정 (Spring, maven 프로젝트 기준) Remove prefix에 target 작성 (~~~.jar 앞의 target을 지워주는 역할) exec command에는 다음과 같이 작성. sudo kill -15 $(sudo lsof -t -i:8080) nohup java -jar ~~~ &gt; nohup.out 2 &gt; &amp;1 &amp; 첫째 줄의 코드는 기존에 실행되고있던 애플리케이션을 중단한 후 새로 시작하기 위해 넣은 코드이다. 만약 이 코드를 넣지 않는다면 두번째 배포시에 이미 8080포트가 이용 중이라서 실행할 수 없다는 에러메세지가 출력된다. 시간이 충분하다면 첫째 줄을 넣지않고 진행 해보는 것을 추천한다 :) 그리고 첫째 줄의 lsof 명령어는 사용되고 있는 포트를 출력해주는 명령인데 기본 제공되는 명령어가 아니므로 apt나 yum을 이용해서 별도 설치해주어야 한다. 또한 kill -15가 아닌 -9도 사용가능한데 -15는 terminate, -9는 kill을 뜻한다. 이 둘의 차이는 terminate의 경우 이미 할당된 프로세스를 전부 마무리하고 종료시키지만 kill은 그 즉시 종료시킨다. 따라서 -9 보다는 -15가 권장된다. ( ~미리 업데이트를 공지한 경우에는 -9가 더 편할 것 같긴하다…~ ) 둘째 줄의 코드는 jar를 백그라운드로 실행함과 동시에 프로세스가 끝나도 종료하지말고 표준 출력과 표준 에러 출력을 모두 nohup.out 파일로 리다이렉션하는 코드이다. 이와 관련된 내용은 더욱 자세히 설명해주신 다른 분의 블로그를 참조바란다. Github Webhook 설정 github webhook은 설정이 간편하다. github의 repository에서 settings - webhooks - add webhook으로 새로운 hook을 만들어준다. payload url은 jenkins 인스턴스의 ip:port를 복사 붙여넣기 해준 후, 뒤에 추가로 api를 작성해준다. 이 api는 jenkins에서 빌드 유발에 github hook trigger를 체크하고 나왔던 url링크를 수정해줬다면 그와 똑같이 적어야한다. 수정해주지 않았다면 디폴트로 github-webhook일 것이다. 이는 추후에 바뀔 수 있으니 참고만 바란다. 그리고 content type은 json으로 해주면 마무리된다. 아마 json 이외의 타입으로 전송될 경우는 적다고 생각하지만 혹시 다른 타입으로(ex. xml) 전송한다면 변경해 주어야한다. 보완사항 여기까지 하면 자동 배포까지는 완료된다. 하지만 jenkins의 exec command를 이렇게 작성할 경우 모든 서버 인스턴스가 동시에 업데이트에 들어가게된다. 그러면 배포가 진행 중인 동안에는 서비스가 끊기게되어 무중단 배포가 이루어지지 않는다. 이를 해결하기위해 간단하게 롤링업데이트 하는 방법을 생각해보았다. 만약 서버 인스턴스가 3개라고 가정했을 때, 두번째와 세번째 exec command의 첫째줄에 sleep 명령어를 추가해서 한 박자 늦게 시작하게끔하면 모든 인스턴스가 동시에 업데이트에 들어가지 않고 차례대로 진행하게 된다. 하지만 이 방법은 야매(?)스러운 방법이라 문제점이 많다. 예를들어, 제대로된 롤링 업데이트의 경우 두번째나 세번째 인스턴스의 업데이트를 진행하던 도중 에러가 발생하여 진행하지 못하게되면 첫째 인스턴스도 다시 롤백되어야 한다. 그렇지만 이 코드는 롤백의 기능은 없다. 더불어 sleep 커맨드로 시간을 지정해주기 때문에 (ex. 30) 업데이트에 걸리는 시간을 예측해야하는데, 이를 잘 맞추지 못할 경우 필요 이상으로 시간이 오래걸리거나 모든 인스턴스가 업데이트에 들어가있는 사태가 벌어질 수도 있다. 그래서 이는 수정이 필요한데 jenkins에서 롤링 업데이트나 카나리 업데이트를 하는 방법을 조금 생각해 봐야할 것 같다. 또한 요새는 유연한 확장을 위해 애플리케이션을 서비스 단위로 쪼개서 개발하는 MSA (Micro Service Architecher)가 일반적이다. 그리고 그 중심 기술이 컨테이너 기술인데, 이를 이용하기 위해서는 Docker가 필수라고 할 수 있다. Dockerhub에 automated build라는 것을 이용해서 github의 소스를 컨테이너화 시킬수 있다고 하던데 오늘 정리한 github webhook과 jenkins사이에 dockerhub automated build를 추가하여 컨테이너화 된 애플리케이션을 jenkins가 배포할 수 있도록 하는 방법도 공부해봐야겠다."
  },"/questions/2021-04-15-git-in-work.html": {
    "title": "Git, 실무에선 어떻게 쓰일까",
    "keywords": "Questions",
    "url": "/questions/2021-04-15-git-in-work.html",
    "body": "Git, 실무에선 어떻게 쓰일까 요새 스터디를 하며 git을 코드 리뷰용 툴로 이용하고 있다. (그전엔 그냥 원격 저장소 같은 느낌으로…) 스터디 전에 이용하던 것과 차이는 여러 사람들과 한 레포지토리를 공유하며 쓰고 있는 점인데 이 과정에서 그동안 애매하게 알고있던 브랜치와 PR (Pull Request)를 사용하며 진정한 git에 대해 알아가는 중이다. 이 두가지에 대한 의문이 풀리기 시작하면서 오픈 소스 기여에 대한 로망도 점점 가까워지는 것 같다. 그렇게 git을 쓰며 알아가던 중, 실무에서는 어떨지 궁금증이 생겨났다. PR 후, 새로 바뀐 코드에 대해 모든 것을 체크할까? 테스트코드는? 충돌이 일어났다면 어떻게 대처할까? FE와 BE는 동시에 개발할까, 순서를 두고 개발할까? PR 후, 모든 코드를 체크할까? 정답은 YES다. 그렇기 때문에 더욱 commit 주기가 중요하다고 한다. 대부분 기능 별로 나눠서 commit을 하고 서비스 단위로 묶어서 통합테스트를 진행한다고 한다(단위 테스트는 기능 commit 전 실행). 그렇다면 테스트코드들은? 다 같이 묶여서 배포되는건가?? 라고 순간 생각했는데 패키징 할 때 테스트코드는 포함되지 않는다는 것을 깜빡했다. 이러한 과정을 상상하던 중, 실무에서는 PR을 보고 merge를 결정하는 과정이 어떻게 흘러가고 누구의 결정에 따르는지 궁금해졌다. 아마 개발팀장을 맡고 있는 사람일 것이라 얼핏 생각이 듦과 동시에 PR 산에 갇힌 모습이 상상되어 조금 숙연해졌다. 개발팀에 들어 가게 되면 다른 팀원들도 생각하는 팀원이 되자. 충돌이 일어났다면 어떻게 대처할까? 물론 그런 일은 없어야 마땅하지만, 만약 여러 사람이 하나의 파일을 불가피하게 수정한 경우에는 충돌이 일어나게 된다. 예를들어 A라는 파일을 세 사람이 각각 A$ , A; , A^ 로 수정한 경우는 충돌이 일어나도 A$;^로 전부 합치면 그만이지만, 만약 A!라는 파일을 세 사람이 A , A! , A% 같이 빼고 더하고가 난무한 경우에는 어떻게 충돌을 처리할까? 너무 극단적인 경우라서 아마 자주 안일어날 것 같긴하지만… 예상으로는 이런 경우가 일어난다면 급하게 팀 회의가 열리지 않을까…? 때에 따라서 A라는 파일이 크게 종속적인 파일 (ex. pom.xml)일 경우에는 세 사람 중 한 두명은 짰던 코드를 전부 갈아 엎어야 할 텐데 생각만해도 너무 끔찍하다. 프로그래머로써 커뮤니케이션이 얼마나 중요한지 생각하게되는 질문거리 였다. FE와 BE는 동시에 개발할까, 순서를 두고 개발할까? 이 부분이 궁금했던 이유는, 보통 하나의 프로젝트를 FE와 BE로 나눠서 개발한다고 가정하였을 때, 동시에 개발 할 경우 사전에 기획된 대로 개발 할 뿐, 이미 있는 FE 폼이나 BE 폼을 참고하며 개발할 순 없게된다. 아마 기준을 두고 여러번 회고를 진행하겠지만, 회고에서 두 파트가 어긋나는 경우가 많으면 굉장히 비효율적이게 된다. 그렇다고 FE나 BE를 먼저 개발하고 그 후 다른 파트를 개발하게 된다면, FE가 개발할 땐 BE가 상대적으로 업무량이 적고 그 반대의 경우도 마찬가지가 된다. 이에 대해서 실무에서는 FE와 BE의 개발이 어떤 흐름으로 흘러갈 지 관심을 가지게 되었다. 오늘 질문의 종착점 자문을 구하자 여기저기 물어보면 답은 나오겠지만… 듣고 궁금증이 해결되었다고 해서 시원스럽진 않을 것 같다. 경험이 중요하니깐. 취업을 하자 제일 확실한 해결책이지만 내 마음대로 되는 것은 아니니까 (주륵) 오픈소스 기여에 참가하자 아무래도 실무자들도 많이 기여를 하기 때문에 git을 통해 branch와 PR을 이용하는 과정은 비슷하게 갈 것이라 예상된다. 취업이 내 마음대로 안된다는 점을 생각하면 현실적으로 제일 가능한 선택지인데, 참여하기 위한 허들이 얼마나 높을지가 관건이다."
  },"/planning-and-reviewing/2021-04-10-Coding-Test.html": {
    "title": "Coding Test를 준비하며 느낀점",
    "keywords": "Planning-And-Reviewing",
    "url": "/planning-and-reviewing/2021-04-10-Coding-Test.html",
    "body": "Coding Test를 준비하며 느낀점 코딩테스트를 처음 만난지 일 년. 본격적으로 준비한 지 한 달. 2020년 초, 한창 졸업에 필요한 필수 항목들을 체크하며 채워나갈 때 아이디어 및 프로젝트 공모전에 나간적이 있다. 공모전에 참여했었다는 것 만으로도 졸업 조건 중 하나에 충족했기 때문에 자신은 없지만 같은 학년의 친구들과 도전하게 되었고, 참여를 위한 첫 관문으로 사업계획서 제출과 코딩테스트 응시를 만나게 되었다. (~지금에서야 드는 생각이지만 조금 더 열정을 가지고 공부해가며 참가했으면 어땠을까… 반성하자ㅠㅠㅠ~) 사업계획서는 모두의 아이디어를 쥐어짜내어 어떻게든 썼지만 코딩테스트라는 벽에 무심코 덤빈 결과는 배드엔딩이었다. 그 후 1년이 지나, 궁극적인 목적은 취업 관문을 통과하기 위해서지만 코딩테스트 대회도 준비하며 여러가지 문제를 풀고 풀어나가는 방법을 정리해나가기 시작했다. 그리고 그 과정이 3월 23일 스코페 2021 준비부터 시작해 한 달 정도가 지난 지금, 회고를 작성해보고자 한다. Java는 잠시 내려두고 Python을 잡다. HTML, CSS, Python, C++, JS 등 많은 언어를 만났지만 학교 커리큘럼 상 앞의 언어들은 수박 겉 핥기 식으로 넘어갈 뿐, 주로 배우고 쓴 언어는 Java였다. 특히 Python은 VSCode가 없었던 때에 다른 IDE를 쓰지않고 cmd 창 같은 곳에다가 코딩을 했는데 (정확히 기억 안나지만 아마 Python 기본 툴) 이것의 단점이 코드를 여러 줄 타이핑하고 실행했는데 오타가 나면 다시 처음부터 써야하는 번거로움이 있었기에 Python은 내게 좋은 기억은 아니었다. 그렇게 안좋은 기억을 준 Python을 왜 다시 찾게 되었냐하면… 본격적으로 코딩테스트 준비를 시작하고 첫 문제를 Java로 풀어서 냈다. 20 줄 정도를 써서 내었는데, Python 풀이를 보니 단 2줄로 끝나는 것이다. Java를 사용하며 해줘야 했던 번거로운 과정들이 Python에서는 기본으로 제공되는 기능을 간략한 문법으로 풀어내다보니 코드는 엄청 짧아지고 풀이시간 또한 굉장히 단축될 것이 뻔했다. 더이상 고민할 필요는 없었다. 바로 Java를 잠시 내려두고 Python으로 코딩테스트를 준비하기 시작했다. Python을 다시 배우기 시작하며 느낀 점을 한마디로 표현하자면, 예전 기억과는 달리 굉장히 배우기 쉽고 간편한 언어였다. IDE가 받쳐주니 Python의 장점이 몸소 느껴지기 시작했고, 코드업 기초 100제를 풀며 Python의 기초를 알아갔다. 잠깐 여담이지만, 새로운 언어를 배워보고자 할 때 코딩테스트 기초문제를 풀어보는 것이 굉장히 좋은 방법이라는 것을 이때 깨달았다. 100문제는 큰 어려움 없이 잘 풀었으나 아직 Python을 Python답게 쓰고 있지 못하는 것 같았다. 그래서 프로그래머스의 고득점 Kit 문제를 유형별로 풀어보며, 다른 사람들의 코드를 많이 참고하며 공부했다. 특히 문제를 풀어나가는 방법을 유형별로 익혀가는 것이 제일 도움이 되었다. 어떤 개념이 코딩테스트에서 가장 큰 도움이 되었나? 코딩테스트를 보다 보면 프로그래머스 기준 2단계 ~ 3단계 문제를 풀기 위해 자료구조와 알고리즘을 어느 정도 알아야하는데, 이 부분을 학교에서 관심있게 공부했던 것이 큰 도움이 되었다. 특히 자신이 2단계와 3단계 사이의 벽을 못넘고 있다면, stack과 queue를 자유자재로 쓸 수 있게 공부하는 것을 추천한다. 3단계부터는 그래프, DFS, BFS, DP 등의 문제가 주로 나오는데, 대부분이 dictionary를 이용한 hash와 queue, stack 그리고 2차원 배열을 주로 사용하므로 이 개념들을 아는 것이 제일 도움이 된 것 같다. 다음 목표는? 이제 3단계 까지는 어찌저찌 풀어내긴 하지만 풀이 시간에서는 부족함이 많다. 때론 2단계 문제도 첫 단추를 잘못 꿰어서 헤매다가 시간이 꽤 걸리기도 한다. 좀 더 문제를 많이 풀어보고 경험을 쌓아서 프로그래머스에서 제공하는 실력체크 3단계 뱃지까지 취득하는 것이 다음 목표이다."
  },"/back-end-study/2021-04-01-Nginx-(Load-Balancer).html": {
    "title": "Nginx Load Balancer",
    "keywords": "Back-End-Study",
    "url": "/back-end-study/2021-04-01-Nginx-(Load-Balancer).html",
    "body": "Nginx 설치 및 간단 Load Balancer 설정 Nginx 란? 엔진엑스(Nginx)는 Igor Sysoev라는 러시아 개발자가 동시접속 처리에 특화된 웹 서버 프로그램이다. Apache보다 동작이 단순하고, 전달자 역할만 하기 때문에 동시접속 처리에 특화되어 있다. Nginx의 역할 중 가장 중요한 두 가지 역할은 다음과 같다. 정적 파일을 처리하는 HTTP 서버로서의 역할 응용프로그램 서버에 요청을 보내는 리버스 프록시로서의 역할 또한 Nginx는 비동기 처리 방식(Event-Drive) 방식을 채택하고 있다. 동기(Synchronous) : A가 B에게 데이터를 요청했을 때, 이 요청에 따른 응답을 주어야만 A가 다시 작업 처리가 가능 (하나의 요청, 하나의 작업에 충실) 비동기(Asynchronous) : A의 요청을 B가 즉시 주지 않아도, A의 유휴시간으로 또 다른 작업 처리가 가능한 방식 동기식 처리 방식과 비동기식 처리 방식의 차이 Nginx 설치 작성자는 Ubuntu 20.04 환경에서 실행했습니다. sudo apt-get install nginx 명령어로 간단하게 설치 할 수 있다. Nginx 실행 설치 된 상태 그대로 sudo systemctl start nginx 를 통해 Daemon을 실행해주면 바로 기본 index 페이지가 뜬다. 이 상태 그대로 웹 서버처럼 사용할 수도 있지만, Load Balacer 로써 사용하기 위해서는 nginx.conf 파일을 설정해주어야 한다. sudo vi /etc/nginx/nginx.conf 명령어를 통해 nginx의 설정을 수정하자. Load Balancing 설정을 하기 전, Nginx 공식 기술 문서를 참조 할 것을 추천한다. 내가 듣는 강의에서 사용한 예시 코드 : upstream cpu-bound-app { server {instance_1번의_ip}:8080 weight=100 max_fails=3 fail_timeout=3s; server {instance_2번의_ip}:8080 weight=100 max_fails=3 fail_timeout=3s; server {instance_3번의_ip}:8080 weight=100 max_fails=3 fail_timeout=3s; } location / { proxy_pass http://cpu-bound-app; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } #code by foo 설정을 이렇게 바꿔주었다면 sudo systemctl restart nginx 또는 sudo systemctl reload nginx를 통해 Daemon을 재시작 해주어야 한다. 여기까지하면 Nginx가 알아서 Load Balancing을 잘 해줄것 같지만 테스트해보면 그렇지가 않다. 이를 해결하기위한 과정이 아래 이어진다. TroubleShooting 위와 같이 설정을 마친 후, 다시 nginx에 접속해보면 404 Error 페이지가 맞이해준다. 어떤 문제점이 있었는지 확인하기 위해 nginx 서버의 log 파일을 볼 필요가 있다. sudo tail -f /var/log/nginx/error.log; 명령어를 통해 log를 열어보면, connect() 가 실패했고 에러코드 13: 권한없음 이라고 적힌 에러가 많이 나와있다. 이 에러코드를 복사해서 구글링하면 해결하기 위해 아래 명령어를 사용해야한다고 나온다. sudo setsebool -P httpd_can_network_connect on 이는 connect() 메서드가 기본적으로 사용을 막아놓았기 때문에 발생하는 에러였다. 위 명령어를 실행해주고 다시 접속해보면 정상적으로 nginx가 load balancing 하고 있는 것을 확인 할 수 있다."
  },"/trouble-shooting/2021-03-29-GCP-%EC%9D%B4%EC%9A%A9-%EC%A4%91-yum-install-error.html": {
    "title": "GCP 이용 중 yum install error",
    "keywords": "Trouble-Shooting",
    "url": "/trouble-shooting/2021-03-29-GCP-%EC%9D%B4%EC%9A%A9-%EC%A4%91-yum-install-error.html",
    "body": "Issue Google Cloud Platform에서 Compute Engine 서비스로 CentOS 7 이용 중 yum install error가 발생하여 설치가 안되는 상황. yum signature could not be verified로 통칭되는 오류이다. Steps to reproduce the issue 그저 GCP나 AWS Instance를 LinuxOS로 생성하고 yum, apt 등의 패키지 매니저를 통해 install 하려하면 등장한다. stackoverflow에서 같은 오류를 겪은 사람이 어떻게 해결했나 보러 다녔는데, 항상 발생하는 것은 아니고 가끔씩 gpg key에 의해 발생하는 오류로 추정된다고 하더라. GCP나 AWS Linux를 이용하다보면 드물게 발생한다는데…. 이런건 왜 나만 걸리는 걸까 TroubleShooting 처음 접근은 에러메세지에서 추천하는 방법 중 --disablerepo 옵션을 사용한 방법을 실행하였지만 같은 오류의 반복이었다. 이 부분에서 에러메세지를 자세히 보지 않고 간과한 것이 빙 돌아가는 길의 시작이었다. 두번째로 gpg key를 몽땅 갈아엎고 새로 업데이트 시키자고 생각하여 rpm --import &lt;(curl -s -L https://packages.cloud.google.com/yum/doc/yum-key.gpg) 명령으로 새로운 키를 받아와보려했으나 write body failed가 뜨면서 어려워졌다. 다시 오류메세지를 제대로 읽어보자고 생각, --disablerepo=google-cloud-sdk 옵션을 이용하여 에러메세지를 띄우고 보았더니 google-compute-engine 이라는 레포지토리가 하나 더 걸려서 에러가 지속되었던 것. 최종적으로 sudo yum install wget --disablerepo=google-cloud-sdk,google-compute-engine -y 처럼 걸리는 레포지토리들을 전부 --disablerepo 옵션으로 걸러내주니 정상 작동하였다. 예에~ 하지만 명령마다 저 옵션을 써주기엔 너무 번거로워서 조금 더 시간을 내어 완벽하게 오류를 고쳐보려했으나 (처절한 에러와의 싸움.png) 고쳐지는건 나였고, 구글의 프로그래머 분들에게 떠넘기기로 했다. 헤헤 추가 글을 적으면서 문득 떠오른것이, ‘어 그럼 gpg 키 검사 안하면 되는거 아닌가?’ 그래서 --nogpgcheck 옵션으로 해봤더니 또 잘된다. gpg key는 무엇이고 왜 없어도 잘 동작되는지 궁금해지기 시작했다."
  },"/back-end-study/2021-03-29-Jenkins.html": {
    "title": "Jenkins 사용 정리",
    "keywords": "Back-End-Study",
    "url": "/back-end-study/2021-03-29-Jenkins.html",
    "body": "Jenkins 개념, 설치 및 실행 정리 Jenkins 란? 소프트웨어 개발 시 지속적으로 통합서비스를 제공해주는, CI(Continuous Integration)툴 이다. Git과 같은 버전관리시스템과 연동하여 커밋을 감지하면 자동빌드/배포 되도록 설정할 수 있다. 하지만 커밋은 매우 빈번히 일어나기 때문에 작업이 큐잉되어 빌드되는 것이 일반적이다. 이러한 자동화 작업은 다음과 같은 이점이 있다. 프로젝트 표준 컴파일 환경에서의 컴파일 오류 검출 자동화 테스트 수행 정적 코드 분석에 의한 코딩 규약 준수여부 확인 프로파일링 툴을 이용한 성능 변화 모니터링 결합 테스트 환경에 대한 배포작업 각종 배치 작업의 간략화 Jenkins는 또한 플러그인을 간단히 인스톨 할 수 있는 기능을 제공하고 있어 자신에게 필요한 기능을 손쉽게 사용가능하다. 혹여나 기능이 없다면 파이썬과 같은 스크립트를 이용해 추가 할 수도 있다. Jenkins 설치 jenkins는 yum 명령어로 설치되지 않는다. 그래서 다음 명령들을 실행해야한다. OS 환경은 CentOS7 이다. sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo sudo tpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key 그다음 sudo yum install jenkins 명령을 치면 설치가 잘 된다. 설치 후에는 잊지말고 sudo systemctl start jenkins로 daemon을 실행시켜주자. 또는 DockerHub에서 이미지를 다운받아 Docker로 실행하는 방법도 있다. 이에 대한 설명은 생략한다. Jenkins 실행 jenkins는 기본으로 8080포트를 사용한다. 클라우드 환경이라면 외부 IP:8080, 로컬이라면 localhost:8080 으로 접속하면 비밀번호를 입력하라는 창이 나온다. 창에 나와있는 경로로 가면 비밀번호가 있으니 복붙해서 접속하면 플러그인 설치가이드가 나온다. 왼쪽은 일반설치, 오른쪽은 매뉴얼 설치인데 만약 설치 중 계속 installation fail이 뜨고 그냥 넘어가거나 재시도 해야하는 상황이 되었다면 버전문제이므로 최신 버전을 설치해주자. 설치가 완료 후 jenkins가 안전하게 worker instance에 접속할 수 있도록, 그리고 다른 접속을 막기위해 ssh-key-pair를 이용해 비대칭키를 만들어 적용한다. 우선 jenkins server terminal에서 ssh-keygen -t rsa -f ~/.ssh/id_rsa 명령어로 키페어를 만들어준다. 그 후 .ssh 디렉토리로 들어가보면 .pub이 있는 것과 없는 것 두 가지가 있을텐데 .pub이 있는 것을 복사해서 worker instance의 ~/.ssh/authorized_keys 파일에 붙여넣기 해준다. 그 후 /.ssh 디렉토리의 권한은 700으로, /.ssh/authorized_keys는 600으로 바꿔준다. 클라우드 환경인 경우, 클라우드 제공자가 ssh 키를 따로 보관하는 경우가 있으니 참고 배포하기 이제 jenkins 관리 페이지로 가서 Publish over SSH 플러그인을 설치 해준 후, 환경설정에서 맨아래 Publish over SSH의 Key 칸에 .pub이 없었던 개인키를 복붙해주고 ssh servers에 Name은 임의로, Hostname은 worker instance의 주소, Username은 worker instance에서 사용하는 user 이름, Remote Directory는 웬만하면 home 디렉토리로 설정해서 저장. 다시 메인으로 가서 새로운 아이템 만들기, 프리스타일 프로젝트, 맨 아래 ‘빌드 후 조치’. ‘고급’ 클릭 후 verbose output in console을 체크, 그 다음 Exec command에 컨테이너 실행명령을 적어주자. 다음, 메인화면에서 Build Now를 누르면 빌드가 시작된다. 빌드가 완료되면 왼쪽아래에 무엇인가 떴을 것이다. #1 같이 뜬 것의 이름 옆 삼각형을 눌러 Console Output을 볼 수 있는데 이것을 보며 TroubleShooting 하면 된다. 만약 스프링 같은 웹서버를 구동할 경우 정상 작동 중인데 빌드가 완료되지 않았다고 표시가 된다. 그럴 땐 다시 구성으로 가서 Exec command의 코드를 nohup ... &gt; /dev/null 2&gt;&amp;1 &amp; 로 수정하고 다시 실행 해보자. nohup과 맨 뒤 &amp;는 이 명령을 백그라운드로 실행시키겠다는 뜻이고, /dev/null 2&gt;&amp;1명령은 표준에러를 표준출력으로 리다이렉트 시켜준다는 명령이다. 또한 도커로 컨테이너를 실행한다면 /var/run/docker.sock 관련 권한 오류가 나오는데, 이는 worker instance로 가서 chmod 666 /var/run/docker.sock 명령어를 실행시키면 해결된다."
  },"/planning-and-reviewing/2021-03-23-SCOFE2021-Review.html": {
    "title": "SCOFE2021 Review",
    "keywords": "Planning-And-Reviewing",
    "url": "/planning-and-reviewing/2021-03-23-SCOFE2021-Review.html",
    "body": "21년 3월 23일 스코페 2021 리뷰 - 나에게 “대회” 란 대학생 때의 나는 대회에 대해, “입상의 가능성이 있을 때 나가는 것.” 이라는 생각을 가지고 있었다. 항상 준비가 어느정도 되야 나가는 의미가 있다고 생각했다. 지금 생각하면 너무 아쉽고 왜 그렇게 생각하고 있었을까 하는 후회가 든다. 나도 믿기지 않는 사실이, 그동안 IT와 관련된 공모전, 대회, 콘테스트, 페스티벌 같은 활동에 관심도 없었고 참여도 하지 않았다는 것이다. 어쩌면 그때의 나는 전공이 컴공임에도 불구하고 코딩에 흥미를 못느끼고 있었던 것일지도 모른다. 하지만 이번 스코페를 통해 인식이 바뀌었다. 그동안 참가에 의의를 두고 출전해서 뿌듯함을 느낀 것으론 축구, 게임 밖에 없었는데 코딩에서도 그것을 느끼니 굉장히 오묘한, 끝난 후에도 아드레날린이 솟구치는 느낌이랄까. 새로운 시작을 열리는 경험이 되었다. 그래서 이제는 대회에 대해 “새로운 분야에 발을 들이기 위한 엔진.” 이라고 표현하고 싶다. - 소감? 재밌었다. 내 상황이 준비되지 않았음에도 불구하고. 대회를 마쳤음에도 마음에 불씨가 계속 피어올라 있었다. 주력 언어인 Java를 두고 코딩테스트를 위해 Python을 쓰자라고 결정하고, 3일동안 코드업 기초 100제와 모의테스트 문제만 풀고 출전하게 되었다. 3일동안 공부하며, 그리고 대회에서 코딩테스트를 진행하는 동안 Python의 능력에 감탄하고 하나씩 찾아 배워나가는 것이 제일 즐거웠다. 또한 오히려 유형에 대한 준비가 안되어있었기 때문에 한 문제 한 문제 풀어나갈 때마다 희열을 느꼈다. 하지만 많이 부족하다는 것도 동시에 느꼈다. 내가 나아가야할 길이 얼마나 남았는지 알 수 있었다. 조금 더 정확하게, 조금 더 간결하게, 조금 더 빠르게 더 많은 유형을 풀어보며 업그레이드 해야겠다고 생각했다. - 다음 계획 4월 2일에 프로그래머스에서 열리는 DevMatching이 있다. 이번에도 마음은 가볍게 가져갈 생각이지만, 준비만큼은 조금 더 확실하게 하려고 한다. 코딩테스트가 될 수도 있고, 과제테스트가 될 수도 있다고 하던데 어느 쪽이던지 우선 신청을 하고 소식을 받아보면서 준비해 나갈 계획이다. 어쩌면 졸업을 했기에 이렇게 경험을 할 수 있는 것 이었을지도. 이것이 나에게 계획된 시간표이기 때문에 지금부터 전력을 다 하면 되는 것이다."
  },"/back-end-study/2021-03-15-Cpu-Stress-Test-2.html": {
    "title": "Cpu Stress Test -2",
    "keywords": "Back-End-Study",
    "url": "/back-end-study/2021-03-15-Cpu-Stress-Test-2.html",
    "body": "목차 Artillery 설치 Artillery를 사용해서 CPU Stress Test Artillery 설치 오늘은 Artillery를 사용해서 우리가 만든 웹서버가 얼마나 많은 트래픽을 소화해낼 수 있는지 테스트해보았다. 내가 수많은 테스트 툴 중에서 Artillery를 선택한 이유는 다음 두 가지이다. Http, websocket 프로토콜 지원 시각화가 잘 되어 있어 한눈에 보기 편한 리포트 페이지 제공 특히 리포트 페이지는 처음 열어보았을 때 너무 깔끔해서 놀랐을 정도. 더 자세한 Artillery에 대한 정보 구글 번역을 이용할 경우 artillery가 “포병”으로 해석되서 나오니 주의. node.js , npm 설치 Artillery를 설치하기 위해서는 node.js와 npm이 필수이다. 각각 아래의 명령어를 입력해서 설치. sudo apt install npm sudo apt install nodejs Artillery 설치 다음으로 Artillery 공식 문서 를 참조하여 Artillery를 설치하자. 공식 문서에서는 -g (전역 설치) 옵션과 함께 1.6 버전을 설치하지만 나는 조금 다르게 설치하였다. sudo npm install -D artillery@latest 프로세스가 끝나면 설치가 완료되었는지 artillery dino 명령을 사용해서 확인하였다. Artillery를 사용해서 CPU Stress Test 지금부터는 Artillery를 본격적으로 사용하기 위해 VSCode를 사용하였다. 우선 open folder로 artillery 관련 산출물을 관리하기 위한 디렉토리를 하나 생성. new file로 어떻게 테스트를 진행할지 구성하는 artillery-cpu-test.yaml 파일을 생성. 다음으로 Artillery 공식 문서 에서 core-concept 탭에 가서 맨 아래있는 기본 yaml 템플릿을 전부 복사 후 artillery-cpu-test.yaml에 붙여넣기 하고, 약간의 수정을 거쳤다. 수정 한 이유는 hash/{input} api를 통해 get 명령만 사용할 것 이므로 나머지는 전부 날렸다. (물론 중요한 두 부분은 빼고) address : 웹 서버 주소 (클라우드 외부 IP) durability : 지속시간 ?? : 지속시간동안 1초마다 트래픽 증가 수치 준비가 끝났으니 웹 서버를 켜주고, 아래 명령을 입력하여 테스트를 시작하자. artillery run --output report.json artillery-cpu-test.yaml 테스트가 끝나고나면 왼쪽에 지정해준 이름대로 json 파일이 생성된다. 나는 report.json으로 했으니 테스트가 잘 끝났음을 보여준다. 하지만 json 파일 만으로는 테스트가 어땠는지 파악하기 어려우니 아래 명령어를 입력해서 리포트 페이지를 열었다. artillery report report.json 이와같이 깔끔하게 테스트 결과를 시각화해준다. 보아하니 서버가 그냥저냥 거뜬히 견딜 정도의 트래픽이었나보다. 여기서 특히 P95의 수치에 집중하였다. 서버의 비용과 유저의 만족도 간 최고의 효율을 낼 수 있는 비율이 95%라고 한다. 다음은 서버가 적당히 스트레스를 받을 정도의 부하를 줘 보자. 트래픽을 초당 15로 수정 후 report-15.json으로 다시 진행하였다. 900 정도의 트래픽이 몰리니 에러도 나오고 지연 시간이 꽤 오래 걸리기 시작했다. 거의 마지막에는 1만 ms 를 넘겼는데 이는 대충 10초가 지나야 응답이 온다는 뜻이다. 내 입장에선 10 초 정도면 나름 견딜만 한 시간인데 전체 유저 입장에서 보면 적당한 것 일까 하는 의문이 든다. 비용이 많다면 유저를 만족시키는 정확한 성능을 찾는 것이 중요하겠지만, 비용이 제한된 상황에서라면 어떤 지침을 따르는 것이 맞을지 찾아보았는데 다음 세 가지를 만족하는 방향을 찾으면 된다고 한다. 예상 TPS보다 여유롭게. 예상 3000이라면 적어도 4000이상 기대 Latency를 만족할 때 까지 Scale-out을 해도 성능이 높아지지 않는다면 병목을 의심 이상으로 CPU Stress Test 포스팅을 마무리한다. 처음해본 배포, 처음해본 부하 테스트 였는데 작지만 내가 만든 애플리케이션이 클라우드를 통해 배포되고, 많은 트래픽이 오는 상황을 예측해볼 수 있다니 두근거리는 경험이었다. 후에 이 경험을 살려 실제로 내가 회사나 개인 프로젝트를 띄워서 관리 중일 때 얼마나 많은 트래픽을 견뎌낼 수 있을지 테스트할 수 있을 것 같다."
  },"/trouble-shooting/2021-03-05-KT-Cloud-TroubleShooting.html": {
    "title": "KT Cloud TroubleShooting (ubuntu account)",
    "keywords": "Trouble-Shooting",
    "url": "/trouble-shooting/2021-03-05-KT-Cloud-TroubleShooting.html",
    "body": "리눅스 OS (ubuntu)에서 KT Cloud Instance OS (ubuntu) ssh 접속 문제 Description KT Cloud의 D1을 이용하다가, 사용 중인 노트북의 os가 ubuntu였기 때문에 시작된 문제이다. ssh 접속 시 openssh의 버전에 따라 client에서의 접속이 차단 될 수 있다는 점을 새로 알게되었고, KT Cloud 기술 문서에서 오점을 찾아내었다. Error Code ubuntu에서 ssh 명령어로 원격 접속하려 했으나 Connection Closed 됨. Windows에서 putty를 이용해 접속하려 했으나 not existing current key 출력 됨. TroubleShooting Connection Closed가 되었다는건 연결은 되나 무언가의 오류로 인해 연결을 막고 있다는 뜻으로 해석했다. 그리고 언제나 나를 괴롭혀왔던 버전문제… 이번에도 역시나였다. (운이 좋았다. 헤헤) client와 server의 버전이 다를 경우 ssh 접속이 차단되는지 구글링을 시작하였고, 얻어낸 답은 ssh는 client의 버전이 server 버전보다 높을 경우 Connection Closed로 접속을 차단한다. 그래서 Windows로 먼저 접속해서 openssh의 버전을 update하고 다시 ubuntu로 시도해보기로 했다. 그런데 Windows로 접속 시도 중 예상치 못하게 키페어가 없다는 오류가 뜨면서 막막해지기 시작했다. 공식 기술 문서를 다시 읽어보고 여러번 시도해보고 실패하고 반복하던 도중, 기술 문서에는 ubuntu os 는 계정명을 ubuntu으로 사용하라는 문장을 발견. 설마… 하는 느낌이 와서 계정을 ubuntu가 아닌 root로 바꿔보았다. CentOS는 root 로그인이 가능한데 ubuntu는 따로 언급이 없다는 것이 흠칫했다. 접속에 성공했다….. 허무했다. 하지만 기술 문서에서 보완할 점을 하나 찾았다는 것이 꽤 뿌듯했다. 앞으로 클라우드 서버를 자주 이용하게 될테니 신고식 한 번한 셈 쳐야겠다."
  },"/back-end-study/2021-03-03-Cpu-Stress-Test.html": {
    "title": "Cpu Stress Test -1",
    "keywords": "Back-End-Study",
    "url": "/back-end-study/2021-03-03-Cpu-Stress-Test.html",
    "body": "목차 I/O Burst, I/O Bound VS CPU Burst, CPU Bound Hash를 이용해서 CPU를 극단적으로 사용하기 (feat. Spring Boot) GCP 인스턴스에 배포 I/O Burst, I/O Bound VS Cpu Burst, Cpu Bound I/O Burst와 I/O Bound는 무엇이고, Cpu Burst와 Cpu Bound는 무엇일까? 이를 설명하기 위해서는 우선 프로세스가 처리되는 과정을 먼저 알아야한다. 그 전에 프로그램과 프로세스의 차이점부터 짚고 넘어가자. 프로그램 : 하드에 저장되어 있는 Application 프로세스 : 메모리에 적재되어 있는 프로그램 이렇게 메모리에 적재되어 처리를 기다리고 있는 프로세스들을 처리하는 순서를 프로세스 스케줄링이라 한다. 이 프로세스 스케줄링에 따라 프로세스들은 CPU에 의해 처리되는데, 이 때 CPU에 I/O(input/output)되는 시간을 I/O Burst, CPU가 프로세스를 처리하는 시간을 CPU Burst라고 한다. 그리고 I/O Burst가 큰 프로그램을 I/O Bound, CPU Burst가 큰 프로그램을 CPU Bound라고 한다. 나는 GCP CPU 테스트를 위해 CPU Burst가 높은 간단한 프로그램을 만들었다. Hash를 이용해서 CPU를 극단적으로 사용하기 아래와 같은 코드를 참조하여 hash (md5)를 이용하는 프로그램을 작성하였다. hash가 무엇인지 궁금하다면? 해시에 대해 ’'’java import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import javax.xml.bind.DatatypeConverter; import java.security.MessageDigest; import java.security.NoSuchAlgorithmException; @RestController public class HashController { @RequestMapping(\"/hash/{input}\") public String getDigest(@PathVariable(\"input\") String input) throws NoSuchAlgorithmException { for(int i = 0; i &lt; 100_000; i++) { input = getMD5Digest(input); } return input; } @RequestMapping(\"/hello\") public String hello() { return \"hello\"; } private String getMD5Digest(String input) throws NoSuchAlgorithmException { MessageDigest md = MessageDigest.getInstance(\"MD5\"); md.update(input.getBytes()); byte[] digest = md.digest(); String myHash = DatatypeConverter .printHexBinary(digest).toUpperCase(); return myHash; } } ''' 참조 : https://github.com/lleellee0/cpu-bound-application –추가사항 : 충돌 방지를 위해 application.properties 파일에서 server.port 를 80으로 설정해줘야 한다. /hash/{input} 과 /hello 두 가지 매핑이 있고, input에 들어온 문자열을 10만번 해시 연산한 결과를 반환한다. ERROR FIX JDK8이 아닌 다른 버전을 쓸 경우 DataTypeConverter를 찾지 못하는 오류가 있었다. 나는 11로 하고 있었기 때문에 해당 패키지가 제외되어있던 상태라 의존성을 추가해주어야 했다. ’'’xml javax.xml.bind jaxb-api 2.3.1 ‘’’ GCP 인스턴스에 배포 1 intelliJ 우측에서 maven 탭을 통해 .jar 파일을 만든다. (Maven -&gt; Lifecycle -&gt; deploy 더블클릭으로 생성!) 익숙했던 maven 이지만 처음 알았던 내용인 만큼 조금 충격이었달까… 그동안 배포를 한번도 안해봐서 .jar 파일을 만들 생각도, 만들어 볼 못했지만 이렇게해서 배포가 이루어 진다는 것을 배웠을 때 굉장히 희열을 느꼈다. ( 생각해보니까 패키지 매니저인데 의존성 관리만 하고 패키징을 제대로 해본 적이 없었네… ) 2 깃허브 Repo에 push. 3 GCP 인스턴스에 가서 wget [repo에 저장된 .jar파일 주소]을 사용해 인스턴스에 다운로드. 나는 CentOS를 설치했기 때문에 yum install wget을 통해 wget 설치하였다. GCP 는 한 계정당 3달간 300달러 무료 크레딧을 지급한다. 초과 시 과금되니 계정관리 필수 !! GCP 인스턴스 생성에 대해서는 생략한다. 4 sudo java -jar [다운받은 .jar 파일명] 명령어를 통해 실행 테스트! 정상적으로 실행되는 모습이다. 스트레스 테스트를 위한 과정 중 중간까지 왔다. 이제 준비가 다 되었고 스트레스 테스트하는 일만 남았는데 준비하는 과정만해도 새로이 배우는 것들이 많아서 굉장히 재미있었다. 다음 포스팅에서는 스트레스 테스트 툴 중 하나인 Artillery를 이용하여 테스트 결과를 출력해보자."
  }}
